---
title: "Introduction to JAGS #1"
output: html_notebook
---

This tutorial teaches you how to use JAGS for model-based machine learning. We will cover basics: how to use JAGS from R, how to write a Bayesian model using JAGS, and how to perform a sanity check of whether your JAGS model is doing what you want it to do. 

# Setting Up
As I noted above, we use R and JAGS to learn basics of model-based machine learning (ML). If you have not, download and install JAGS from this page. You should be able to use JAGS from all popular environments (Windows, Mac, Ubuntu).

Once you have both R and JAGS installed, download and install the folloiwng R libraries: `R2jags`, `runjags`, and `mcmcplots`. These libraries are used to call JAGS from within R. Once you install the libraries, import them:

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(R2jags)
library(runjags)
library(mcmcplots)
```

# Modeling Task Duration using JAGS
In this tutorial, we will make a model for estimating a task duration of a person completing a task. When I say a "task," imagine a person completing, say, a simple image transcription task for crowdsourcing; a person sees an image, and types the text that is shown in the image.

Model-based ML allows you to combine prior knowledge, often based on a common sense and existing theory, to let you infer the quantity of the interest (a task duration in our case). The prior knowledge is encoded as a model, which is [a set of assumptions](http://www.mbmlbook.com/LearningSkills_A_model_is_a_set_of_assumptions.html). For instance, when modeling a task duration, we could make certain assumpations:

1. A task duration should be a positive continuous number.
2. A task duration would follow a long-tail distribution with a certain mode and a spread.
3. (For a simplicity,) a task duration does not depend on the person's ability or the difficulty of the image transcription task. (We will come back to this in the future tutorial.) 

We often use a plate diagram or a factor graph to represent this kind of model. For instance, I drew one diagram that models the task duration with a `gamma distribution`. See the accompanying tutorial for the diagram. Note that, such a model is just one of many plausible models that could represent the assumptions listed above. The question of "what is the better model?" is a topic of model comparison that we should discuss in the future tutorial. 

The plate diagram in the tutorial could be translated into the following JAGS model.

```{r}
model {

  for (i in 1:N) {
    duration[i] ~ dgamma( shape, rate )
  }

  # Parameterizing the above gamma distribution using
  # mode and sd (standard deviation)

  shape <- 1 + mode * rate
  rate <- (mode + sqrt( mode^2 + 4 *sd^2 ) ) / ( 2 * sd^2 )
  
  mode ~ dunif(0, 100)
  sd ~ dunif(0, 100)
}
```

Notice that the first two assumptions (1 and 2) are represented with the fact that I use `dgamma( shape, rate )`. A gamma distribution is a long-tail distribution that is parameterized by two values `shape` and `rate`, and it can only take positive range. Also notice that the gamma distribution's parameters do not depend on anything, showing that (assumption 3) the distribution does not depend on factors like task difficulty.

Shape and rate are described using `mode` and `sd` (standard deviation) for intuitiveness. The long tail distribution is peaked at the `mode` and has some spread parameterized by `sd`. To represent our ignorance about where the `mode` is and how much spread (`sd`) we have, they take priors a wide uniform distribution.

# Generating Data from the Model
We are often interested in using model-based ML to learn parameters (i.e., obtain posterior distributions of `mode` and `sd`) from the data. But before using the real data of task durations, we are often interested in getting a sense of how the model behaves given synthetic data. One way to create a synthetic data is to use the model that you wrote and pass parameters that you decide. 

```{r}
data_generation_txt <- '
model {
  for (i in 1:N) {
    duration[i] ~ dgamma( shape, rate )
  }

  # Parameterizing the above gamma distribution using
  # mode and sd (standard deviation)

  shape <- 1 + mode * rate
  rate <- (mode + sqrt( mode^2 + 4 *sd^2 ) ) / ( 2 * sd^2 )
  
  # mode ~ dunif(0, 100) # Instead of defining `mode` and `sd` as a random variable,
  # sd ~ dunif(0, 100)   # pass hard coded values for these variables.
}
'
```

See that the `mode` and `sd` that we previously defined as random variables that follow a uniform distribution are now commented out. In the following code, we pass a value for `mode` and `sd`, then sample 100 x 1,000 values of `duration`s that are generated by this model (and the hard coded parameters).

```{r}
N <- 1000
mode <- 10
sd <- 1

data <- list(
  N=N,
  mode=mode,
  sd=sd
)

out <- run.jags(
  data_generation_txt, 
  data = data,
  monitor = c("duration"),
  sample = 100,
  n.chains = 1)

```

Now, let's take a look at the summary statistics and the visualization of the generated data.

```{r}
Simulated <- coda::as.mcmc(out)
dim(Simulated)
dat <- as.vector(Simulated)
summary(dat)

# TODO. ADD a nicer visualization

hist(dat)
```

The summary above correctly shows that the mode is near 10, as we specified. And you can check that `sd` is around 1 if you call `sd(dat)`.

Now, let's use the synthetic data to see if we can let the model learn the parameters `mode` and `sd`. The code below takes the model that we defined and the (synthetic) data to learn the two parameters. The algorithm that we use here is called MCMC (or Gibbs sampling more specifically). We will talk more about MCMC in the future tutorial.

Notice that we are we have a line that says `parameters <- c("shape", "rate", "mode", "sd")`. This line allows us to monitor the parameters that we want to keep an eye on. 

```{r}
model <- 
paste("
  model {
  
    for (i in 1:N) {
      duration[i] ~ dgamma( shape, rate )
    }
  
    # Parameterizing the above gamma distribution using
    # mode and sd (standard deviation)
  
    shape <- 1 + mode * rate
    rate <- (mode + sqrt( mode^2 + 4 *sd^2 ) ) / ( 2 * sd^2 )
    
    # Now these parameters are back to random variables.
    mode ~ dunif(0, 100)
    sd ~ dunif(0, 100)
  }
")

model_filename <- "01-jags-intro.jags"
writeLines(model, model_filename)

jags.data <- list(
  duration = dat,
  N = N
)

inits <- function(){
  list()
} 

# parameters monitored
parameters <- c("shape", "rate", "mode", "sd")

# MCMC settings
ni <- 10000
nt <- 6
nb <- 5000
nc <- 2

# call JAGS from R
res <- jags(
  jags.data, 
  inits, 
  parameters, 
  model_filename, 
  n.chains = nc, 
  n.thin = nt, 
  n.iter = ni, 
  n.burnin = nb, 
  working.directory = getwd())


```
# Result
Now, let's see the results.
```{r}
# summarize posteriors
print(res, digits = 3)
```

The visualization that is created as you execute the following code is called a trace plot. It visualizes whether the parameters that you are monitoring are converging. 
```{r}
# trace plots
traplot(res, c("shape", "rate", "mode", "sd"))
```

And you can see the sampled parameters of our interest. See that the parameters are close to the true values (e.g., `mode` is near 10 and `sd` is near 1).
```{r}
# posterior distributions
denplot(res, c("shape", "rate", "mode", "sd"))
```

# References
* http://doingbayesiandataanalysis.blogspot.com/2012/08/gamma-likelihood-parameterized-by-mode.html
* https://en.wikipedia.org/wiki/Gamma_distribution
* http://www.mbmlbook.com/LearningSkills_A_model_is_a_set_of_assumptions.html
